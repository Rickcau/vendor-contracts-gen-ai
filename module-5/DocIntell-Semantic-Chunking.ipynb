{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are going to take a look at a concept called semantic chunking and how this can be acheived with Azure AI Document Intelligence which would be using with a RAG workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from azure.storage.blob import BlobSasPermissions\n",
    "from azure.storage.blob import generate_blob_sas\n",
    "from datetime import datetime, timedelta, UTC  # Added UTC\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Get root directory path\n",
    "root_dir = Path().absolute().parent\n",
    "env_path = root_dir / '.env'\n",
    "\n",
    "# Load .env from root\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "print(f\"Loaded .env from {env_path}\")\n",
    "\n",
    "# Azure AI Document Intelligence setup\n",
    "file_path = \"path/to/your/document.pdf\"\n",
    "endpoint = os.getenv('FORM_RECOGNIZER_ENDPOINT')\n",
    "key = os.getenv('FORM_RECOGNIZER_KEY')\n",
    "\n",
    "# Azure Storage settings\n",
    "storage_account_name = os.getenv('STORAGE_ACCOUNT_NAME')\n",
    "storage_account_key = os.getenv('STORAGE_ACCOUNT_KEY')\n",
    "container_name = \"source\"\n",
    "input_filename = \"VendorAgreement-Fabrikam-5004432.pdf\"\n",
    "\n",
    "# Azure Blob Storage setup (assuming the document is already in a blob with a public URL)\n",
    "# document_url = \"https://your_storage_account.blob.core.windows.net/your_container/your_document.pdf\"\n",
    "\n",
    "# Initialize the Document Intelligence client\n",
    "doc_intelligence_client = DocumentIntelligenceClient(endpoint, AzureKeyCredential(key))\n",
    "\n",
    "\n",
    "\n",
    "def generate_sas_url(blob_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full URL with SAS token for a specific blob\n",
    "    \"\"\"\n",
    "    # Define the permissions for the SAS token\n",
    "    sas_permissions = BlobSasPermissions(read=True)\n",
    "    \n",
    "    # Set token expiry time using timezone-aware datetime\n",
    "    expiry_time = datetime.now(UTC) + timedelta(hours=1)\n",
    "    \n",
    "    # Generate the SAS token\n",
    "    sas_token = generate_blob_sas(\n",
    "        account_name=storage_account_name,\n",
    "        account_key=storage_account_key,\n",
    "        container_name=container_name,\n",
    "        blob_name=blob_name,\n",
    "        permission=sas_permissions,\n",
    "        expiry=expiry_time\n",
    "    )\n",
    "    \n",
    "    # Construct the full URL including the SAS token\n",
    "    blob_url = f\"https://{storage_account_name}.blob.core.windows.net/{container_name}/{blob_name}?{sas_token}\"\n",
    "    \n",
    "    return blob_url\n",
    "\n",
    "document_url = generate_sas_url(input_filename)\n",
    "\n",
    "# Begin analysis with the document URL\n",
    "poller = doc_intelligence_client.begin_analyze_document(\n",
    "    model_id=\"prebuilt-layout\",\n",
    "    analyze_request={\"urlSource\": document_url}\n",
    ")\n",
    "\n",
    "# Get the result of the analysis\n",
    "result = poller.result()\n",
    "\n",
    "# Extract the content as markdown\n",
    "markdown_content = result.content\n",
    "print(f\"\\nMarkdown Content:\\n{markdown_content}\\n\")\n",
    "\n",
    "# Define headers for splitting\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "# Initialize the MarkdownHeaderTextSplitter\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Split the document into chunks based on markdown headers\n",
    "splits = text_splitter.split_text(markdown_content)\n",
    "\n",
    "# Print the splits\n",
    "for split in splits:\n",
    "    print(f\"Header: {split.metadata.get('header', 'No Header')}\")\n",
    "    print(f\"Content: {split.page_content[:100]}...\")  # Print first 100 \n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vendor-contracts-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
