{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a few tests for sanity checks...  Let's make sure we don't have any issues with the imports and the loading of the .env variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env from c:\\Users\\rickcau\\source\\repos\\vendor-contracts-gen-ai\\.env\n",
      "API Key: 8PVz****************************Isv1\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex\n",
    ")\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Any\n",
    "#import lib for pypdf2\n",
    "\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from datetime import datetime\n",
    "import os  \n",
    "from dotenv import load_dotenv  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "\n",
    "from openai import AzureOpenAI  \n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import itertools\n",
    "\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv \n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Get root directory path\n",
    "root_dir = Path().absolute().parent\n",
    "env_path = root_dir / '.env'\n",
    "\n",
    "# Load .env from root\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "print(f\"Loaded .env from {env_path}\")\n",
    "# Access variables\n",
    "aoai_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "\n",
    "print(f\"API Key: {  api_key[:4] + '*' * 28 + api_key[-4:] }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure all our keys are being properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORAGE_ACCOUNT_CONNECTION_STRING: Defa****************************.net\n",
      "Container Name: vendor-contracts\n",
      "STORAGE_ACCOUNT_NAME: aoaistorage\n",
      "FORM_RECOGNIZER_ENDPOINT: https://aoai-formrecognizer.cognitiveservices.azure.com/\n",
      "FORM_RECOGNIZER_KEY: 8PVz****************************Isv1\n",
      "AZURE_SEARCH_ENDPOINT: https://aoai-search.search.windows.net/\n",
      "AZURE_SEARCH_KEY: 8PVz****************************Isv1\n",
      "AZURE_SEARCH_INDEX: aoai-index\n",
      "AZURE_OPENAI_DEPLOYMENT_NAME: gpt-4o\n",
      "AZURE_OPENAI_ENDPOINT: https://aoai-service-centralus-rdc.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "connect_str = os.getenv(\"STORAGE_ACCOUNT_CONNECTION_STRING\")\n",
    "container_name = \"vendor-contracts\"\n",
    "storage_account_name = os.getenv(\"STORAGE_ACCOUNT_NAME\")\n",
    "\n",
    "form_recognizer_endpoint = os.getenv(\"FORM_RECOGNIZER_ENDPOINT\")\n",
    "form_recognizer_key = os.getenv(\"FORM_RECOGNIZER_KEY\")\n",
    "\n",
    "ai_search_endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "ai_search_key = os.environ[\"AZURE_SEARCH_KEY\"]\n",
    "ai_search_index = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "\n",
    "# Azure OpenAI\n",
    "aoai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "aoai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "endpoint = form_recognizer_endpoint\n",
    "\n",
    "print(f\"STORAGE_ACCOUNT_CONNECTION_STRING: {  connect_str[:4] + '*' * 28 + connect_str[-4:] }\")\n",
    "print(f\"Container Name: {  container_name }\")\n",
    "print(f\"STORAGE_ACCOUNT_NAME: {  storage_account_name }\")\n",
    "print(f\"FORM_RECOGNIZER_ENDPOINT: {  form_recognizer_endpoint }\")\n",
    "print(f\"FORM_RECOGNIZER_KEY: {  form_recognizer_key[:4] + '*' * 28 + form_recognizer_key[-4:] }\")\n",
    "print(f\"AZURE_SEARCH_ENDPOINT: {  ai_search_endpoint }\")\n",
    "print(f\"AZURE_SEARCH_KEY: {  ai_search_key[:4] + '*' * 28 + ai_search_key[-4:] }\")\n",
    "print(f\"AZURE_SEARCH_INDEX: {  ai_search_index }\")\n",
    "print(f\"AZURE_OPENAI_DEPLOYMENT_NAME: {  aoai_deployment }\")\n",
    "print(f\"AZURE_OPENAI_ENDPOINT: {  aoai_endpoint }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up a few LLMs as we might want to use different versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = AzureKeyCredential(form_recognizer_key)\n",
    "document_intelligence_client = DocumentIntelligenceClient(endpoint, credential)\n",
    "\n",
    "\n",
    "search_index_client = SearchIndexClient(ai_search_endpoint, AzureKeyCredential(ai_search_key))\n",
    "search_client = SearchClient(ai_search_endpoint, ai_search_index, AzureKeyCredential(ai_search_key))\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "        api_version=\"2023-05-15\"\n",
    "        )\n",
    "\n",
    "primary_llm = AzureChatOpenAI(\n",
    "    azure_deployment=aoai_deployment,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=aoai_key,\n",
    "    azure_endpoint=aoai_endpoint\n",
    ")\n",
    "\n",
    "primary_llm_json = AzureChatOpenAI(\n",
    "    azure_deployment=aoai_deployment,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=aoai_key,\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    ")\n",
    "\n",
    "contract_indexing_prompt = \"\"\"You are an AI assistant. Your job is to read the input contract, \n",
    "and output certain info in valid JSON format. Here is what you should be extracting:\n",
    "\n",
    "1. experienceLevel - either entry, mid, or senior\n",
    "2. jobTitle - the title of the job the resume is for\n",
    "3. skills_and_experience - a succinct list of 3-5 top skills and experiences.   \n",
    "\n",
    "\n",
    "#Examples#\n",
    "\n",
    "User: Dan Giannone\n",
    "CAREER SUMMARY\n",
    "Resourceful and detail-oriented general contractor with 5+ years of experience recruiting and coordinating labor across construction sites.\n",
    "Adept at obtaining permits, inspecting sites, ensuring building code compliance, offering cost estimates, and employing skilled labor.\n",
    "Articulate communicator and effective negotiator with the ability to foster strong relationships with organizational management, key clients, vendors, and team members.\n",
    "PROFESSIONAL EXPERIENCE\n",
    "March 2020 - Present | Precision Pro 5, New York, NY\n",
    "General Contractor\n",
    "· Manage various construction projects with budgets of up to $5M each\n",
    "· Train subcontractors on company standards and protocols\n",
    "· Attained high client satisfaction by optimizing project efficiency and ensuring timely completion\n",
    "· Achieved annual cost control targets in 2020 and 2021 through strategic planning and execution\n",
    "June 2017 - March 2020 | YPC, New York, NY\n",
    "General Contractor\n",
    "· Performed pre-construction inspections and managed post-\n",
    "construction audits for 3+ projects per year\n",
    "· Streamlined project-related functions by developing schedules,\n",
    "overseeing quality control, and ensuring within-budget project\n",
    "completion\n",
    "· Reported progress and project modifications to superintendents\n",
    "and clients\n",
    "· Supervised team of 6 subcontractors to resolve complex issues\n",
    "and prevent unnecessary delays\n",
    "(917) 828-9045\n",
    "eloise.plaza@email.com\n",
    "in\n",
    "linkedin.com/in/eloise-plaza/\n",
    "EDUCATION\n",
    "Bachelor of Science in\n",
    "Construction Management\n",
    "Honors: cum laude (3.6/4.0)\n",
    "Columbia University,\n",
    "New York, NY\n",
    "May 2017\n",
    "SKILLS\n",
    "Cost reduction & elimination\n",
    "Project estimation\n",
    "Residential construction\n",
    "House renovation & remodeling\n",
    "Subcontractor management\n",
    "Workforce planning & scheduling\n",
    "Complex problem-solving\n",
    "Contract negotiation\n",
    "Microsoft Office\n",
    "Google Suite\n",
    "\n",
    "\n",
    "Assistant: {'experienceLevel': 'mid', 'jobTitle': 'General Contractor', 'skills_and_experience': ['Cost reduction & elimination', 'Project estimation', 'Subcontractor management', 'Workforce planning & scheduling', 'Contract negotiation']}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the functions that create the index and populate the index with data.  Notice, how th populate_index has depdencies on othe functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(): \n",
    "    #Check if index exists, return if so\n",
    "    try:\n",
    "        # Try to get the index\n",
    "        search_index_client.get_index(ai_search_index)\n",
    "        # If no exception is raised, the index exists and we return\n",
    "        print(\"Index already exists\")\n",
    "        return\n",
    "    except:\n",
    "        # If an exception is raised, the index does not exist and we continue with the logic to create it\n",
    "        pass\n",
    "\n",
    "    # Rest of your code...\n",
    "\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True),\n",
    "        SimpleField(name=\"date\", type=SearchFieldDataType.DateTimeOffset, filterable=True, facetable=True),\n",
    "        SimpleField(name=\"jobTitle\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SimpleField(name=\"experienceLevel\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"sourceFileName\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchField(name=\"searchVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "\n",
    "    ]\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\"\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    index = SearchIndex(name=ai_search_index, fields=fields,\n",
    "                    vector_search=vector_search)\n",
    "    result = search_index_client.create_or_update_index(index)\n",
    "\n",
    "    print(\"Index has been created\")\n",
    "\n",
    "def generate_embeddings(text, model=\"text-embedding-ada-002\"): # model = \"deployment_name\"\n",
    "    return aoai_client.embeddings.create(input = [text], model=model).data[0].embedding    \n",
    "\n",
    "def read_pdf(input_file):\n",
    "    blob_url = f\"https://{storage_account_name}.blob.core.windows.net/{container_name}/{input_file}\"\n",
    "    analyze_request = {\n",
    "        \"urlSource\": blob_url\n",
    "    }\n",
    "    poller = document_intelligence_client.begin_analyze_document(\"prebuilt-layout\", analyze_request=analyze_request)\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    #print(result.content)\n",
    "    \n",
    "    #read result object into a full text variable\n",
    "    full_text = result.content\n",
    "    print(\"Successfully read the PDF from blob storage with doc intelligence and extracted text.\")\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "# RDC: We may or may not need this function, if we do the prompt will need to be changed\n",
    "def llm_extraction(full_text):\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": contract_indexing_prompt}]\n",
    "    messages.append({\"role\": \"user\", \"content\": full_text})\n",
    "\n",
    "    response = primary_llm_json.invoke(messages)\n",
    "    extraction_json = json.loads(response.content)\n",
    "\n",
    "\n",
    "    return extraction_json\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import uuid\n",
    "\n",
    "def generate_document_id(blob_name):\n",
    "    \"\"\"Generate a unique, deterministic ID for a document.\"\"\"\n",
    "    unique_string = f\"{blob_name}\"  # Use first 100 characters of content for uniqueness\n",
    "    return hashlib.md5(unique_string.encode()).hexdigest()\n",
    "\n",
    "def list_blobs_in_folder(container_client, folder_name):\n",
    "    return [blob for blob in container_client.list_blobs() if blob.name.startswith(folder_name)]\n",
    "\n",
    "def move_blob(source_container_client, destination_container_client, source_blob_name, destination_blob_name):\n",
    "    source_blob = source_container_client.get_blob_client(source_blob_name)\n",
    "    destination_blob = destination_container_client.get_blob_client(destination_blob_name)\n",
    "    \n",
    "    destination_blob.start_copy_from_url(source_blob.url)\n",
    "    source_blob.delete_blob()\n",
    "\n",
    "def populate_index():\n",
    "    print(\"Populating index...\")\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    \n",
    "    stage_blobs = list_blobs_in_folder(container_client, \"source/\")\n",
    "    print(f\"Found {len(stage_blobs)} blobs in the 'source' folder\")\n",
    "    \n",
    "    for blob in stage_blobs:\n",
    "        print(f\"Processing {blob.name}\")\n",
    "        print(blob.name)\n",
    "        \n",
    "        try:\n",
    "            full_text = read_pdf(blob.name)\n",
    "            extraction_json = llm_extraction(full_text)        \n",
    "            experienceLevel = extraction_json[\"experienceLevel\"]\n",
    "            jobTitle = extraction_json[\"jobTitle\"]\n",
    "            skills_and_experience = extraction_json[\"skills_and_experience\"]\n",
    "            skills_and_experience_str = \", \".join(skills_and_experience)\n",
    "            searchVector = generate_embeddings(skills_and_experience_str)\n",
    "            current_date = datetime.now(timezone.utc).isoformat()\n",
    "            document_id = generate_document_id(blob.name)\n",
    "            fileName = os.path.basename(blob.name)\n",
    "            print(f\"Extracted experience level: {experienceLevel}\")\n",
    "            print(f\"Extracted job title: {jobTitle}\")\n",
    "            print(f\"Extracted skills and experience: {skills_and_experience_str}\")\n",
    "            print(f\"Current date: {current_date}\")\n",
    "            \n",
    "            document = {\n",
    "                \"id\": document_id,\n",
    "                \"date\": current_date,\n",
    "                \"jobTitle\": jobTitle,\n",
    "                \"experienceLevel\": experienceLevel,\n",
    "                \"content\": full_text,\n",
    "                \"sourceFileName\": fileName,\n",
    "                \"searchVector\": searchVector\n",
    "            }\n",
    "            \n",
    "            search_client.upload_documents(documents=[document])\n",
    "            \n",
    "            # Move the processed file to the 'processed' folder\n",
    "            destination_blob_name = blob.name.replace(\"source/\", \"processed/\")\n",
    "            move_blob(container_client, container_client, blob.name, destination_blob_name)\n",
    "            \n",
    "            print(f\"Successfully processed and moved {blob.name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {blob.name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's test the indexing and the populating of the index..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #reset_processed_files()\n",
    "\n",
    "    create_index()\n",
    "\n",
    "    populate_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vendor-contracts-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
